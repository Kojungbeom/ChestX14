{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dd0aeea1c40c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "r50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_categories = {\n",
    "    'Atelectasis': 0,\n",
    "    'Cardiomegaly': 1,\n",
    "    'Effusion': 2,\n",
    "    'Infiltration': 3,\n",
    "    'Mass': 4,\n",
    "    'Nodule': 5,\n",
    "    'Pneumonia': 6,\n",
    "    'Pneumothorax': 7,\n",
    "    'Consolidation': 8,\n",
    "    'Edema': 9,\n",
    "    'Emphysema': 10,\n",
    "    'Fibrosis': 11,\n",
    "    'Pleural_Thickening': 12,\n",
    "    'Hernia': 13,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = os.path.join(root_dir, 'images')\n",
    "index_dir = os.path.join(root_dir, 'train_label.csv')\n",
    "classes = pd.read_csv(index_dir, header=None,nrows=1).iloc[0, :].to_numpy()[1:9]\n",
    "label_index = pd.read_csv(index_dir, header=0)\n",
    "bbox_index = pd.read_csv(os.path.join(root_dir, 'BBox_List_2017.csv'), header=0)\n",
    "transform = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_dir = os.path.join(root_dir, 'train_label.csv')\n",
    "val_index_dir = os.path.join(root_dir, 'val_label.csv')\n",
    "test_index_dir = os.path.join(root_dir, 'test_label.csv')\n",
    "\n",
    "train_label_index = pd.read_csv(train_index_dir, header=0)\n",
    "val_label_index = pd.read_csv(val_index_dir, header=0)\n",
    "test_label_index = pd.read_csv(test_index_dir, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(root_dir):\n",
    "    train_index_dir = os.path.join(root_dir, 'train_label.csv')\n",
    "    val_index_dir = os.path.join(root_dir, 'val_label.csv')\n",
    "    test_index_dir = os.path.join(root_dir, 'test_label.csv')\n",
    "    \n",
    "    train_label_index = pd.read_csv(train_index_dir, header=0)\n",
    "    val_label_index = pd.read_csv(val_index_dir, header=0)\n",
    "    test_label_index = pd.read_csv(test_index_dir, header=0)\n",
    "    \n",
    "    train_len = len(train_label_index)\n",
    "    val_len = len(val_label_index)\n",
    "    test_len = len(test_label_index)\n",
    "    dataset_len = train_len + val_len + test_len\n",
    "    print(\"Train(0.7%): \\t{0}\\nValid(0.1%): \\t{1}\\nTest(0.2%): \\t{2}\\nAll: \\t{3}\\n\".format(\n",
    "            train_len, val_len, test_len, dataset_len))\n",
    "    for name in disease_categories.keys():\n",
    "        print(\"## \" + name + \" ##\")\n",
    "        num_train = train_label_index[name].sum()\n",
    "        num_val = val_label_index[name].sum()\n",
    "        num_test = test_label_index[name].sum()\n",
    "        num_sum = num_train + num_val + num_test\n",
    "        print(\"Train: \\t{0}\\nValid: \\t{1}\\nTest: \\t{2}\\nAll: \\t{3}\\n\".format(\n",
    "            num_train, num_val, num_test, num_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(0.7%): \t75731\n",
      "Valid(0.1%): \t10793\n",
      "Test(0.2%): \t25596\n",
      "All: \t112120\n",
      "\n",
      "## Atelectasis ##\n",
      "Train: \t7287\n",
      "Valid: \t993\n",
      "Test: \t3279\n",
      "All: \t11559\n",
      "\n",
      "## Cardiomegaly ##\n",
      "Train: \t1526\n",
      "Valid: \t181\n",
      "Test: \t1069\n",
      "All: \t2776\n",
      "\n",
      "## Effusion ##\n",
      "Train: \t7628\n",
      "Valid: \t1031\n",
      "Test: \t4658\n",
      "All: \t13317\n",
      "\n",
      "## Infiltration ##\n",
      "Train: \t12078\n",
      "Valid: \t1704\n",
      "Test: \t6112\n",
      "All: \t19894\n",
      "\n",
      "## Mass ##\n",
      "Train: \t3567\n",
      "Valid: \t467\n",
      "Test: \t1748\n",
      "All: \t5782\n",
      "\n",
      "## Nodule ##\n",
      "Train: \t4128\n",
      "Valid: \t580\n",
      "Test: \t1623\n",
      "All: \t6331\n",
      "\n",
      "## Pneumonia ##\n",
      "Train: \t767\n",
      "Valid: \t109\n",
      "Test: \t555\n",
      "All: \t1431\n",
      "\n",
      "## Pneumothorax ##\n",
      "Train: \t2308\n",
      "Valid: \t329\n",
      "Test: \t2665\n",
      "All: \t5302\n",
      "\n",
      "## Consolidation ##\n",
      "Train: \t2527\n",
      "Valid: \t325\n",
      "Test: \t1815\n",
      "All: \t4667\n",
      "\n",
      "## Edema ##\n",
      "Train: \t1191\n",
      "Valid: \t187\n",
      "Test: \t925\n",
      "All: \t2303\n",
      "\n",
      "## Emphysema ##\n",
      "Train: \t1264\n",
      "Valid: \t159\n",
      "Test: \t1093\n",
      "All: \t2516\n",
      "\n",
      "## Fibrosis ##\n",
      "Train: \t1086\n",
      "Valid: \t165\n",
      "Test: \t435\n",
      "All: \t1686\n",
      "\n",
      "## Pleural_Thickening ##\n",
      "Train: \t1973\n",
      "Valid: \t269\n",
      "Test: \t1143\n",
      "All: \t3385\n",
      "\n",
      "## Hernia ##\n",
      "Train: \t119\n",
      "Valid: \t22\n",
      "Test: \t86\n",
      "All: \t227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_info(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXR8_train(Dataset):\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.index_dir = os.path.join(root_dir, 'train_label.csv')\n",
    "        self.classes = pd.read_csv(self.index_dir, header=None,nrows=1).iloc[0, :].to_numpy()[1:9]\n",
    "        self.label_index = pd.read_csv(self.index_dir, header=0)\n",
    "        #self.bbox_index = pd.read_csv(os.path.join(root_dir, 'BBox_List_2017.csv'), header=0)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.label_index)*0.1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = self.label_index.iloc[idx, 0]\n",
    "        img_dir = os.path.join(self.image_dir, name)\n",
    "        image = Image.open(img_dir).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.label_index.iloc[idx, 1:9].to_numpy().astype('int')\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "class CXR8_validation(Dataset):\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.index_dir = os.path.join(root_dir, 'val_label.csv')\n",
    "        self.classes = pd.read_csv(self.index_dir, header=None,nrows=1).iloc[0, :].to_numpy()[1:9]\n",
    "        self.label_index = pd.read_csv(self.index_dir, header=0)\n",
    "        #self.bbox_index = pd.read_csv(os.path.join(root_dir, 'BBox_List_2017.csv'), header=0)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.label_index)*0.1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = self.label_index.iloc[idx, 0]\n",
    "        img_dir = os.path.join(self.image_dir, name)\n",
    "        image = Image.open(img_dir).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.label_index.iloc[idx, 1:9].to_numpy().astype('int')\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "mean = [0.50576189]\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(512),\n",
    "    transforms.Normalize(mean, [1.])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(512),\n",
    "    transforms.Normalize(mean, [1.])\n",
    "])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    CXR8_train(root_dir, transform_train),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    CXR8_validation(root_dir, transform_val),\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
       "       'Nodule', 'Pneumonia', 'Pneumothorax'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=2048, out_channels=32):\n",
    "        super(TransitionBlock,self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.avg_pool = nn.AvgPool2d(2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.avg_pool(out)\n",
    "        return out\n",
    "    \n",
    "class LogSumExpPool(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        super(LogSumExpPool, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, feat_map):\n",
    "        \"\"\"\n",
    "        Numerically stable implementation of the operation\n",
    "        Arguments:\n",
    "            feat_map(Tensor): tensor with shape (N, C, H, W)\n",
    "            return(Tensor): tensor with shape (N, C, 1, 1)\n",
    "        \"\"\"\n",
    "        (N, C, H, W) = feat_map.shape\n",
    "\n",
    "        # (N, C, 1, 1) m\n",
    "        m, _ = torch.max(\n",
    "            feat_map, dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)\n",
    "\n",
    "        # (N, C, H, W) value0\n",
    "        value0 = feat_map - m\n",
    "        area = 1.0 / (H * W)\n",
    "        g = self.gamma\n",
    "\n",
    "        # TODO: split dim=(-1, -2) for onnx.export\n",
    "        return m + 1 / g * torch.log(area * torch.sum(\n",
    "            torch.exp(g * value0), dim=(-1, -2), keepdim=True))\n",
    "    \n",
    "\n",
    "class ResModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResModel, self).__init__()\n",
    "        r50 = models.resnet50(pretrained=True)\n",
    "        self.layer0 = nn.Sequential(r50.conv1, r50.bn1, r50.relu, r50.maxpool)\n",
    "        self.layer1 = r50.layer1\n",
    "        self.layer2 = r50.layer2\n",
    "        self.layer3 = r50.layer3\n",
    "        self.layer4 = r50.layer4\n",
    "        self.transition_layer = TransitionBlock()\n",
    "        self.lsepool = LogSumExpPool(gamma=5)\n",
    "        \n",
    "        self.fcl = nn.Linear(32, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer0(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.transition_layer(out)\n",
    "        out = self.lsepool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fcl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, [16/7573], loss: 0.6395\n",
      "epoch 0, [336/7573], loss: 0.6212\n",
      "epoch 0, [656/7573], loss: 0.6545\n",
      "epoch 0, [976/7573], loss: 0.6241\n",
      "epoch 0, [1296/7573], loss: 0.6306\n",
      "epoch 0, [1616/7573], loss: 0.6336\n",
      "epoch 0, [1936/7573], loss: 0.6211\n",
      "epoch 0, [2256/7573], loss: 0.6245\n",
      "epoch 0, [2576/7573], loss: 0.6266\n",
      "epoch 0, [2896/7573], loss: 0.6436\n",
      "epoch 0, [3216/7573], loss: 0.6247\n",
      "epoch 0, [3536/7573], loss: 0.6286\n",
      "epoch 0, [3856/7573], loss: 0.6382\n",
      "epoch 0, [4176/7573], loss: 0.6295\n",
      "epoch 0, [4496/7573], loss: 0.6462\n",
      "epoch 0, [4816/7573], loss: 0.6470\n",
      "epoch 0, [5136/7573], loss: 0.6281\n",
      "epoch 0, [5456/7573], loss: 0.6200\n",
      "epoch 0, [5776/7573], loss: 0.6262\n",
      "epoch 0, [6096/7573], loss: 0.6116\n",
      "epoch 0, [6416/7573], loss: 0.6370\n",
      "epoch 0, [6736/7573], loss: 0.6367\n",
      "epoch 0, [7056/7573], loss: 0.6366\n",
      "epoch 0, [7376/7573], loss: 0.6151\n",
      "epoch 0, [7696/7573], loss: 0.6156\n",
      "epoch 0, [8016/7573], loss: 0.6380\n",
      "epoch 0, [8336/7573], loss: 0.6340\n",
      "epoch 0, [8656/7573], loss: 0.6665\n",
      "epoch 0, [8976/7573], loss: 0.6072\n",
      "epoch 0, [9296/7573], loss: 0.6320\n",
      "epoch 0, [9616/7573], loss: 0.6262\n",
      "epoch 0, [9936/7573], loss: 0.6300\n",
      "epoch 0, [10256/7573], loss: 0.6002\n",
      "epoch 0, [10576/7573], loss: 0.6357\n",
      "epoch 0, [10896/7573], loss: 0.6317\n",
      "epoch 0, [11216/7573], loss: 0.6129\n",
      "epoch 0, [11536/7573], loss: 0.6168\n",
      "epoch 0, [11856/7573], loss: 0.6169\n",
      "epoch 0, [12176/7573], loss: 0.6019\n",
      "epoch 0, [12496/7573], loss: 0.6236\n",
      "epoch 0, [12816/7573], loss: 0.6201\n",
      "epoch 0, [13136/7573], loss: 0.6060\n",
      "epoch 0, [13456/7573], loss: 0.6018\n",
      "epoch 0, [13776/7573], loss: 0.6245\n",
      "epoch 0, [14096/7573], loss: 0.5978\n",
      "epoch 0, [14416/7573], loss: 0.6380\n",
      "epoch 0, [14736/7573], loss: 0.6291\n",
      "epoch 0, [15056/7573], loss: 0.6171\n",
      "Atelectasis: 0.5215  \n",
      "Cardiomegaly: 0.4641  \n",
      "Effusion: 0.4524  \n",
      "Infiltration: 0.4598  \n",
      "Mass: 0.5191  \n",
      "Nodule: 0.4887  \n",
      "Pneumonia: 0.4894  \n",
      "Pneumothorax: 0.5087  \n",
      "\n",
      "\n",
      "Atelectasis: 0.0000  \n",
      "Cardiomegaly: 0.0000  \n",
      "Effusion: 0.0000  \n",
      "Infiltration: 0.0000  \n",
      "Mass: 0.0000  \n",
      "Nodule: 0.0000  \n",
      "Pneumonia: 0.0000  \n",
      "Pneumothorax: 0.0000  \n",
      "\n",
      "\n",
      "epoch 0, Val loss: 0.1532, Val AUC 0.0000\n",
      "epoch 1, [16/7573], loss: 0.6283\n",
      "epoch 1, [336/7573], loss: 0.6095\n",
      "epoch 1, [656/7573], loss: 0.6032\n",
      "epoch 1, [976/7573], loss: 0.6263\n",
      "epoch 1, [1296/7573], loss: 0.6143\n",
      "epoch 1, [1616/7573], loss: 0.6203\n",
      "epoch 1, [1936/7573], loss: 0.6183\n",
      "epoch 1, [2256/7573], loss: 0.6258\n",
      "epoch 1, [2576/7573], loss: 0.6225\n",
      "epoch 1, [2896/7573], loss: 0.6175\n",
      "epoch 1, [3216/7573], loss: 0.6248\n",
      "epoch 1, [3536/7573], loss: 0.6031\n",
      "epoch 1, [3856/7573], loss: 0.6013\n",
      "epoch 1, [4176/7573], loss: 0.6069\n",
      "epoch 1, [4496/7573], loss: 0.6204\n",
      "epoch 1, [4816/7573], loss: 0.6110\n",
      "epoch 1, [5136/7573], loss: 0.6268\n",
      "epoch 1, [5456/7573], loss: 0.6203\n",
      "epoch 1, [5776/7573], loss: 0.6273\n",
      "epoch 1, [6096/7573], loss: 0.5942\n",
      "epoch 1, [6416/7573], loss: 0.5939\n",
      "epoch 1, [6736/7573], loss: 0.6091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7650160cf939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#loss_function = nn.CrossEntropyLoss(weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.7/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    189\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch1.7/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retains_grad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n\u001b[1;32m    949\u001b[0m                           \u001b[0;34m\"attribute won't be populated during autograd.backward(). If you indeed want the gradient \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = ResModel()\n",
    "net.cuda()\n",
    "num_epochs = 100\n",
    "gamma = 5\n",
    "learning_rate = 1e-6\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_auc_ave = 0.0\n",
    "since = time.time()\n",
    "best_model_wts = net.state_dict()\n",
    "best_auc = []\n",
    "iter_num = 0\n",
    "model_save_dir = './savedModels'\n",
    "data_root_dir = './dataset'\n",
    "class_names = train_loader.dataset.classes\n",
    "log_dir = './runs'\n",
    "model_name = 'myNet'\n",
    "\n",
    "writer = SummaryWriter(log_dir=os.path.join(log_dir, model_name),comment=model_name)\n",
    "input_tensor = torch.Tensor(1, 3, 512, 512).cuda()\n",
    "writer.add_graph(net, input_tensor)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    train_auc = 0.0\n",
    "    output_list = []\n",
    "    label_list = []\n",
    "    net.train()\n",
    "    # Iterate over data.\n",
    "    \n",
    "    for idx, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        #calculate weight for loss\n",
    "        P = 0\n",
    "        N = 0\n",
    "        for label in labels:\n",
    "            for v in label:\n",
    "                if int(v) == 1: P += 1\n",
    "                else: N += 1\n",
    "        if P!=0 and N!=0:\n",
    "            BP = (P + N)/P\n",
    "            BN = (P + N)/N\n",
    "            weights = torch.tensor([BP, BN], dtype=torch.float).cuda()\n",
    "        #loss_function = nn.CrossEntropyLoss(weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "\n",
    "        labels = labels.type_as(outputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter_num += 1\n",
    "        \n",
    "        outputs = outputs.detach().to('cpu').numpy()\n",
    "        labels = labels.detach().to('cpu').numpy()\n",
    "        for i in range(outputs.shape[0]):\n",
    "            output_list.append(outputs[i].tolist())\n",
    "            label_list.append(labels[i].tolist())\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(\"epoch {epoch}, [{trained_samples}/{total_samples}], loss: {:.4f}\".format(\n",
    "                loss.item(),\n",
    "                epoch=epoch,\n",
    "                trained_samples=idx * batch_size + len(images),\n",
    "                total_samples=len(train_loader.dataset)))\n",
    "            writer.add_scalar('loss', loss.item()/outputs.shape[0], iter_num)\n",
    "    try:\n",
    "        epoch_auc_ave = roc_auc_score(np.array(label_list), np.array(output_list))\n",
    "        epoch_auc = roc_auc_score(np.array(label_list), np.array(output_list), average=None)\n",
    "    except:\n",
    "        epoch_auc_ave = 0\n",
    "        epoch_auc = [0 for _ in range(len(class_names))]\n",
    "                  \n",
    "    log_str = ''\n",
    "    for i, c in enumerate(class_names):\n",
    "        log_str += '{}: {:.4f}  \\n'.format(c, epoch_auc[i])\n",
    "    log_str += '\\n'\n",
    "    print(log_str)\n",
    "                 \n",
    "    net.eval()\n",
    "    val_auc = 0.0\n",
    "    val_loss = 0.0\n",
    "    output_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for idx, data in enumerate(val_loader):\n",
    "        # get the inputs\n",
    "        images, labels = data\n",
    "        \n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        labels = labels.type_as(outputs)\n",
    "        with torch.no_grad():\n",
    "            loss = loss_function(outputs, labels)\n",
    "        val_loss += loss\n",
    "        outputs = outputs.detach().to('cpu').numpy()\n",
    "        labels = labels.detach().to('cpu').numpy()\n",
    "        for i in range(outputs.shape[0]):\n",
    "            output_list.append(outputs[i].tolist())\n",
    "            label_list.append(labels[i].tolist())\n",
    "        \n",
    "    try:\n",
    "        epoch_auc_ave = roc_auc_score(np.array(label_list), np.array(output_list))\n",
    "        epoch_auc = roc_auc_score(np.array(label_list), np.array(output_list), average=None)\n",
    "        print(epoch_auc)\n",
    "    except:\n",
    "        epoch_auc_ave = 0\n",
    "        epoch_auc = [0 for _ in range(len(class_names))]\n",
    "                  \n",
    "    log_str = ''\n",
    "    for i, c in enumerate(class_names):\n",
    "        log_str += '{}: {:.4f}  \\n'.format(c, epoch_auc[i])\n",
    "    log_str += '\\n'\n",
    "    print(log_str)\n",
    "    writer.add_text('log', log_str, iter_num)\n",
    "            \n",
    "    print(\"epoch {}, Val loss: {:.4f}, Val AUC {:.4f}\".format(epoch,\n",
    "                                                         val_loss.item() / len(val_loader.dataset),\n",
    "                                                         val_auc / len(val_loader.dataset)))\n",
    "    writer.add_scalar('loss', val_loss / len(val_loader.dataset), iter_num)\n",
    "    writer.add_scalar('auc', epoch_auc_ave, iter_num)\n",
    "    \n",
    "    if epoch_auc_ave > best_auc_ave:\n",
    "        best_auc = epoch_auc\n",
    "        best_auc_ave = epoch_auc_ave\n",
    "        best_model_wts = net.state_dict()\n",
    "        model_dir = os.path.join(model_save_dir, model_name+'.pth')\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            os.makedirs(model_save_dir)\n",
    "        torch.save(net.state_dict(), model_dir)\n",
    "        print('Model saved to %s'%(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
